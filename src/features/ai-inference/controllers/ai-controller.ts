// AI ì¶”ë¡  ì»¨íŠ¸ë¡¤ëŸ¬ - Gemma 3 4B ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ

import { LlmInference, FilesetResolver } from '@mediapipe/tasks-genai';
import { AIAnalysisResult, AIModelConfig, AIModelStatus, ModelDownloadProgress, LearningSnapshot } from '../types/ai-types';
import { getPromptTemplate, AI_PROMPTS, CURRENT_PROMPT, getBaseExamples } from '../config/ai-prompts';
import { AVAILABLE_MODELS, DEFAULT_MODEL_ID } from '../config/model-registry';
import { AIResponseParser } from '../process/ai-response-parser';
import { OPFSFileManager } from '../process/opfs-file-manager';
import { LearningDataManager } from '../process/learning-data-manager';
import { SnapshotManager } from '../process/snapshot-manager';
import { DOWNLOAD_TIMEOUT_MS } from '../config/opfs-config';


export class AIController {
  private llm: LlmInference | null = null;
  private modelStatus: AIModelStatus = {
    state: 1 // 1: ìºì‹œì—†ìŒ/ë¡œë”©ì•ˆë¨
  };
  private isLearning: boolean = false; // í•™ìŠµ ì¤‘ë³µ ë°©ì§€

  // âœ¨ ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
  private currentModelId: string = DEFAULT_MODEL_ID;
  private downloadProgress: ModelDownloadProgress | null = null;
  private downloadAbortController: AbortController | null = null;
  private readonly fullConfig: AIModelConfig;

  // âœ¨ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ - CURRENT_PROMPT ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸°í™”
  private currentPromptName: keyof typeof AI_PROMPTS;


  // AI ì¶”ë¡  ë™ì‹œì„± ì œì–´
  private isAnalyzing = false;
  private analysisQueue: Array<{
    voiceInput: string;
    resolve: (result: AIAnalysisResult) => void;
    reject: (error: Error) => void;
  }> = [];


  constructor(config: AIModelConfig = {}, modelId?: string) {
    // ëª¨ë¸ ì„ íƒ
    this.currentModelId = modelId || DEFAULT_MODEL_ID;
    const modelInfo = AVAILABLE_MODELS[this.currentModelId];

    if (!modelInfo) {
      console.error(`âŒ [ai-controller] Unknown model ID: ${this.currentModelId}`);
      this.currentModelId = DEFAULT_MODEL_ID;
    }

    // ì„ íƒëœ ëª¨ë¸ì˜ ê¸°ë³¸ ì„¤ì • ì ìš©
    this.fullConfig = {
      ...AVAILABLE_MODELS[this.currentModelId].defaultConfig,
      ...config
    };

    // ëª¨ë¸ ìƒíƒœì— í˜„ì¬ ëª¨ë¸ ID ì €ì¥
    this.modelStatus.currentModelId = this.currentModelId;

    // âœ¨ CURRENT_PROMPTì—ì„œ í‚¤ ì°¾ê¸°
    const currentKey = Object.keys(AI_PROMPTS).find(
      key => AI_PROMPTS[key as keyof typeof AI_PROMPTS] === CURRENT_PROMPT
    ) as keyof typeof AI_PROMPTS;

    this.currentPromptName = currentKey || 'SIMPLE_CLASSIFICATION';

    console.log(`ğŸ¤– [ai-controller] Config initialized for model: ${modelInfo?.name || this.currentModelId}`);
    console.log(`ğŸ¯ [ai-controller] Using prompt: ${this.currentPromptName} (${AI_PROMPTS[this.currentPromptName].name})`);
  }

  /**
   * MediaPipe LLM ì´ˆê¸°í™” (ë¡œì»¬ ìºì‹œ ìš°ì„ )
   */
  async initialize(): Promise<boolean> {
    if (this.modelStatus.state === 3) return true; // ì´ë¯¸ ë¡œë“œë¨
    if (this.modelStatus.state === 2) return false; // ë¡œë”© ì¤‘

    try {
      this.modelStatus.state = 2; // ë¡œë”© ì¤‘
      const startTime = Date.now();
      console.log('ğŸš€ [ai-controller] Initializing AI model from OPFS cache...');

      // OPFSì—ì„œ í˜„ì¬ ëª¨ë¸ íŒŒì¼ í™•ì¸
      console.log(`ğŸ” [ai-controller] Checking if model ${this.currentModelId} exists in OPFS...`);
      const modelExists = await OPFSFileManager.checkModelExists(this.currentModelId);
      console.log(`ğŸ” [ai-controller] Model exists check result: ${modelExists}`);

      if (!modelExists) {
        console.error(`âŒ [ai-controller] No model file found for ${this.currentModelId} in OPFS. Please download the model.`);
        this.modelStatus = { state: 1, currentModelId: this.currentModelId, error: 'Model not found in OPFS.' };
        return false;
      }

      try {
        console.log('ğŸ”— [ai-controller] Loading model from OPFS file...');

        // OPFSì—ì„œ í˜„ì¬ ëª¨ë¸ íŒŒì¼ ë¡œë“œí•˜ì—¬ Object URL ìƒì„±
        const modelFilePath = await OPFSFileManager.getModelFileURL(this.currentModelId);
        console.log(`âœ… [ai-controller] Model file URL for ${this.currentModelId}: ${modelFilePath}`);

        const wasmPath = chrome.runtime.getURL("wasm_files/");
        const genaiFileset = await FilesetResolver.forGenAiTasks(wasmPath);

        // modelAssetPath ë°©ì‹ìœ¼ë¡œ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        this.llm = await LlmInference.createFromOptions(genaiFileset, {
          baseOptions: { modelAssetPath: modelFilePath },
            maxTokens: this.fullConfig.maxTokens!,
            temperature: this.fullConfig.temperature!,
            topK: this.fullConfig.topK!,
            randomSeed: this.fullConfig.randomSeed!
          });

          console.log('âœ… [ai-controller] Model loaded from file path successfully');

      } catch (loadError: any) {
        console.error('âŒ [ai-controller] Failed to load model from OPFS:', loadError);
        this.modelStatus = { state: 1, error: loadError.message };
        throw loadError;
      }

      const loadTime = Date.now() - startTime;
      this.modelStatus = {
        state: 3, // ë¡œë”© ì™„ë£Œ
        loadTime: loadTime
      };

      console.log(`âœ… [ai-controller] SUCCESS: Model loaded from .task bundle in ${loadTime}ms`);
      return true;

    } catch (error: any) {
      this.modelStatus = { state: 1, error: error.message };
      console.error('âŒ [ai-controller] FAILED to initialize from cache:', error);
      return false;
    }
  }

  /**
   * API í† í°ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹±
   */
  /**
   * ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤ì œ ì €ì¥ (modelAssetPath ìŠ¤í‚µí•˜ê³  ë°”ë¡œ ë‹¤ìš´ë¡œë“œ)
   */
  async downloadAndCacheModelAsPath(token: string, modelId?: string): Promise<boolean> {
    console.log('ğŸ’¾ [ai-controller] Skipping modelAssetPath, downloading and caching model for persistent storage...');

    // modelAssetPathëŠ” ì‹¤ì œ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì‚¬ìš©
    return this.downloadAndCacheModel(token, modelId);
  }

  async downloadAndCacheModel(token: string, modelId?: string): Promise<boolean> {
    const targetModelId = modelId || this.currentModelId;
    const modelInfo = AVAILABLE_MODELS[targetModelId];

    if (!modelInfo) {
      console.error(`âŒ [ai-controller] Unknown model ID: ${targetModelId}`);
      return false;
    }

    // ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  ì´ˆê¸°í™”
    this.downloadProgress = {
      modelId: targetModelId,
      progress: 0,
      downloadedBytes: 0,
      totalBytes: 0,
      status: 'downloading'
    };

    this.modelStatus = { state: 2, currentModelId: targetModelId }; // ë¡œë”© ì¤‘

    // AbortControllerë¡œ ê¸´ ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì œì–´
    this.downloadAbortController = new AbortController();
    const downloadTimeout = setTimeout(() => {
      this.downloadAbortController?.abort();
      console.error('âŒ [ai-controller] Download timeout after 10 minutes');
    }, DOWNLOAD_TIMEOUT_MS);

    try {
      const result = await OPFSFileManager.downloadModelToOPFS(
        targetModelId,
        modelInfo,
        token,
        (progress) => {
          this.downloadProgress = progress;
          this.broadcastDownloadProgress();
        },
        this.downloadAbortController.signal
      );

      // íƒ€ì„ì•„ì›ƒ í•´ì œ
      clearTimeout(downloadTimeout);

      if (result) {
        // ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ (ë¡œë“œí•˜ì§€ ì•ŠìŒ)
        this.modelStatus = {
          state: 4, // ìºì‹œë¨, ë¡œë“œ í•„ìš”
          currentModelId: targetModelId
        };
        console.log(`âœ… [ai-controller] Model ${modelInfo.name} download complete and ready to load`);
      }

      // AbortController ì •ë¦¬
      this.downloadAbortController = null;
      return result;

    } catch (error: any) {
      clearTimeout(downloadTimeout); // ì—ëŸ¬ ì‹œì—ë„ íƒ€ì„ì•„ì›ƒ í•´ì œ

      console.error('âŒ [ai-controller] Download failed:', error);

      if (error.name === 'AbortError') {
        console.error('âŒ [ai-controller] Download aborted (timeout or user cancellation)');
        this.modelStatus = { state: 1, currentModelId: targetModelId, error: 'Download cancelled' };

        // ë¶ˆì™„ì „í•œ íŒŒì¼ ì •ë¦¬
        try {
          await OPFSFileManager.deleteModel(targetModelId);
          console.log('ğŸ—‘ï¸ [ai-controller] Incomplete download file cleaned up');
        } catch (cleanupError) {
          console.warn('âš ï¸ [ai-controller] Failed to cleanup incomplete file:', cleanupError);
        }
      } else {
        this.modelStatus = { state: 1, currentModelId: targetModelId, error: error.message };
      }

      // ì—ëŸ¬ ë°œìƒ ì‹œ AbortController ì •ë¦¬
      this.downloadAbortController = null;
      return false;
    }
  }

  /**
   * ë‹¤ìš´ë¡œë“œ ì·¨ì†Œ
   */
  public cancelDownload(): void {
    if (this.downloadAbortController) {
      console.log('ğŸš« [ai-controller] Cancelling download...');
      this.downloadAbortController.abort();

      // ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  ì·¨ì†Œ ìƒíƒœ ì—…ë°ì´íŠ¸
      if (this.downloadProgress) {
        this.downloadProgress.status = 'error';
        this.downloadProgress.error = 'Download cancelled by user';
        this.broadcastDownloadProgress();
      }
    } else {
      console.warn('âš ï¸ [ai-controller] No download to cancel');
    }
  }

  /**
   * OPFS ìºì‹œëœ ëª¨ë¸ ì‚­ì œ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›)
   */
  async deleteCachedModel(modelId?: string): Promise<void> {
    const targetModelId = modelId || this.currentModelId;

    await OPFSFileManager.deleteModel(targetModelId);

    // í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ ì‚­ì œëœ ëª¨ë¸ì´ë©´ ë©”ëª¨ë¦¬ì—ì„œë„ ì œê±°
    if (targetModelId === this.currentModelId) {
      if (this.llm) this.llm = null;
      this.modelStatus = { state: 1, currentModelId: this.currentModelId };
    }
  }

  /**
   * ëª¨ë¸ ìƒíƒœ í™•ì¸ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›, OPFS ì¡´ì¬ ì—¬ë¶€ ë°˜ì˜)
   */
  async getModelStatus(modelId?: string): Promise<AIModelStatus> {
    const targetModelId = modelId || this.currentModelId;

    // í˜„ì¬ ëª¨ë¸ì— ëŒ€í•œ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸
    if (targetModelId === this.currentModelId && this.modelStatus.state === 1) {
      const modelExists = await OPFSFileManager.checkModelExists(targetModelId);
      if (modelExists) {
        this.modelStatus = {
          ...this.modelStatus,
          state: 4,
          currentModelId: targetModelId
        };
        console.log(`ğŸ“¦ [ai-controller] Model ${targetModelId} found in cache but not loaded in memory`);
      }
    }

    // ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
    if (targetModelId !== this.currentModelId) {
      const modelExists = await OPFSFileManager.checkModelExists(targetModelId);
      return {
        state: modelExists ? 4 : 1,
        currentModelId: targetModelId
      };
    }

    return { ...this.modelStatus };
  }

  /**
   * ìŒì„± ëª…ë ¹ ì˜ë„ ë¶„ì„ (ë™ì‹œì„± ì œì–´ í¬í•¨)
   */
  async analyzeIntent(voiceInput: string): Promise<AIAnalysisResult> {
    console.log('ğŸ¯ [ai-controller] Analyzing voice intent with Gemma 3 4B:', voiceInput);

    if (this.modelStatus.state !== 3 || !this.llm) {
      console.log('âš ï¸ [ai-controller] Model not loaded, using fallback analysis');
      throw new Error('AI model is not initialized.');
    }

    // ë™ì‹œì„± ì œì–´: íì— ì¶”ê°€í•˜ê³  ìˆœì°¨ ì²˜ë¦¬
    return new Promise((resolve, reject) => {
      this.analysisQueue.push({ voiceInput, resolve, reject });
      this.processAnalysisQueue();
    });
  }

  private async processAnalysisQueue(): Promise<void> {
    if (this.isAnalyzing || this.analysisQueue.length === 0) {
      return;
    }

    this.isAnalyzing = true;

    while (this.analysisQueue.length > 0) {
      const { voiceInput, resolve, reject } = this.analysisQueue.shift()!;

      try {
        console.log(`ğŸ”„ [ai-controller] Processing analysis (${this.analysisQueue.length} remaining)`);
        const prompt = await this.buildAnalysisPrompt(voiceInput);
        const response = await this.llm!.generateResponse(prompt);
        const result = AIResponseParser.parseAIResponse(response, voiceInput);
        resolve(result);
      } catch (error: any) {
        console.error('âŒ [ai-controller] AI analysis failed:', error);
        reject(error);
      }
    }

    this.isAnalyzing = false;
  }

  /**
   * âœ¨ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë©”ì„œë“œë“¤
   */
  public setPromptTemplate(promptName: keyof typeof AI_PROMPTS): void {
    this.currentPromptName = promptName;
    console.log(`ğŸ”„ [ai-controller] Switched to prompt: ${AI_PROMPTS[promptName].name}`);
  }

  public getCurrentPrompt(): string {
    return AI_PROMPTS[this.currentPromptName].name;
  }

  public getAvailablePrompts(): Array<{name: keyof typeof AI_PROMPTS, description: string}> {
    return Object.entries(AI_PROMPTS).map(([key, value]) => ({
      name: key as keyof typeof AI_PROMPTS,
      description: value.description
    }));
  }

  /**
   * âœ¨ ì„¤ì • íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (CURRENT_PROMPT ê¸°ë°˜)
   * JSON íŒŒì¼ì˜ ê¸°ë³¸ ì˜ˆì‹œ + Chrome Storageì˜ í•™ìŠµëœ ì˜ˆì‹œë¥¼ ê²°í•©
   */
  private async buildAnalysisPrompt(voiceInput: string): Promise<string> {
    const promptTemplate = getPromptTemplate(this.currentPromptName);
    console.log(`ğŸ¯ [ai-controller] Using prompt template: ${promptTemplate.name}`);

    // ê¸°ë³¸ ì˜ˆì‹œë“¤ ë¡œë“œ
    const baseExamples = getBaseExamples();

    // OPFSì—ì„œ ì¶”ê°€ í•™ìŠµëœ ì˜ˆì‹œë“¤ ë¡œë“œ
    const learnedExamples = await LearningDataManager.getLearnedExamples();

    // ëª¨ë“  ì˜ˆì‹œ ê²°í•© (í•™ìŠµëœ ì˜ˆì‹œê°€ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    const allExamples = [...learnedExamples, ...baseExamples];

    console.log(`ğŸ“š [ai-controller] Using ${baseExamples.length} base examples + ${learnedExamples.length} learned examples`);

    return promptTemplate.template(voiceInput, allExamples);
  }


  /**
   * ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ì„ í•™ìŠµ ì˜ˆì‹œë¡œ ì €ì¥ (OPFS íŒŒì¼)
   */
  public async learnFromFailedTests(failedTests: Array<{ command: string; expected: string; description: string }>): Promise<void> {
    // ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if (this.isLearning) {
      console.warn('âš ï¸ [ai-controller] Learning already in progress, skipping...');
      return;
    }

    this.isLearning = true;
    try {
      console.log('ğŸ§  [ai-controller] Learning from failed tests to OPFS...');

      // ğŸ“¸ í•™ìŠµ ì „ ìë™ ìŠ¤ëƒ…ìƒ· ìƒì„±
      const failedCommands = failedTests.map(t => t.command).join(', ');
      const snapshotDescription = `Before learning ${failedTests.length} failed cases: ${failedCommands.substring(0, 100)}${failedCommands.length > 100 ? '...' : ''}`;

      try {
        await SnapshotManager.createSnapshot(snapshotDescription);
        console.log('ğŸ“¸ [ai-controller] Auto-snapshot created before learning');
      } catch (snapshotError) {
        console.warn('âš ï¸ [ai-controller] Failed to create auto-snapshot, but continuing with learning:', snapshotError);
      }

      await LearningDataManager.learnFromFailedTests(failedTests);
    } catch (error) {
      console.error('âŒ [ai-controller] Failed to learn from failed tests:', error);
      throw error;
    } finally {
      this.isLearning = false; // í”Œë˜ê·¸ í•´ì œ
    }
  }








  /**
   * OPFSì—ì„œ í•™ìŠµëœ ì˜ˆì‹œ íŒŒì¼ ì‚­ì œ (í•„ìš”ì‹œ)
   */
  public async clearLearnedExamples(): Promise<void> {
    await LearningDataManager.clearLearnedExamples();
  }

  /**
   * í•™ìŠµëœ ì˜ˆì‹œ í˜„í™© ì¡°íšŒ
   */
  public async getLearnedExamplesStats(): Promise<{count: number, size: number}> {
    return await LearningDataManager.getLearnedExamplesStats();
  }

  /**
   * í˜„ì¬ í•™ìŠµ ë°ì´í„°ì˜ ìŠ¤ëƒ…ìƒ· ìƒì„± (í•™ìŠµ ì „ ë°±ì—…)
   */
  public async createSnapshot(description?: string): Promise<LearningSnapshot> {
    return await SnapshotManager.createSnapshot(description);
  }

  /**
   * íŠ¹ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ë¡¤ë°±
   */
  public async rollbackToSnapshot(snapshotId: string): Promise<boolean> {
    return await SnapshotManager.rollbackToSnapshot(snapshotId);
  }

  /**
   * ëª¨ë“  ìŠ¤ëƒ…ìƒ· ëª©ë¡ ì¡°íšŒ
   */
  public async getSnapshots(): Promise<LearningSnapshot[]> {
    return await SnapshotManager.getSnapshots();
  }

  /**
   * íŠ¹ì • ìŠ¤ëƒ…ìƒ· ì‚­ì œ
   */
  public async deleteSnapshot(snapshotId: string): Promise<boolean> {
    return await SnapshotManager.deleteSnapshot(snapshotId);
  }




  // =============================================================================
  // ğŸŒ ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› ë©”ì„œë“œë“¤
  // =============================================================================

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
   */
  public getAvailableModels() {
    return AVAILABLE_MODELS;
  }

  /**
   * í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ID ë°˜í™˜
   */
  public getCurrentModelId(): string {
    return this.currentModelId;
  }

  /**
   * ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  ë°˜í™˜
   */
  public getDownloadProgress(): ModelDownloadProgress | null {
    return this.downloadProgress;
  }

  /**
   * ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ ì„ UIë¡œ ì‹¤ì‹œê°„ ì „ì†¡
   */
  private broadcastDownloadProgress(): void {
    if (!this.downloadProgress) return;

    try {
      // Background Scriptë¥¼ í†µí•´ UIë¡œ ë©”ì‹œì§€ ì „ì†¡
      chrome.runtime.sendMessage({
        action: 'downloadProgress',
        progress: { ...this.downloadProgress }
      }).catch((error) => {
        // ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ (ë©”ì¸ ë‹¤ìš´ë¡œë“œì— ì˜í–¥ ì—†ìŒ)
        console.warn('âš ï¸ [ai-controller] Failed to broadcast download progress:', error);
      });
    } catch (error) {
      console.warn('âš ï¸ [ai-controller] Failed to broadcast download progress:', error);
    }
  }

  /**
   * ëª¨ë¸ ì „í™˜ (ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ í›„ ìƒˆ ëª¨ë¸ ë¡œë“œ)
   */
  public async switchModel(modelId: string, token?: string): Promise<boolean> {
    const modelInfo = AVAILABLE_MODELS[modelId];
    if (!modelInfo) {
      console.error(`âŒ [ai-controller] Unknown model ID: ${modelId}`);
      return false;
    }

    console.log(`ğŸ”„ [ai-controller] Switching from ${this.currentModelId} to ${modelId}...`);

    // ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
    if (this.llm) {
      this.llm = null;
      console.log('ğŸ“‹ [ai-controller] Previous model unloaded from memory');
    }

    // ìƒˆ ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸
    this.currentModelId = modelId;
    Object.assign(this.fullConfig, modelInfo.defaultConfig);

    // ìƒˆ ëª¨ë¸ ìƒíƒœ ì´ˆê¸°í™”
    this.modelStatus = {
      state: 1,
      currentModelId: modelId
    };

    // ëª¨ë¸ì´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    const modelExists = await OPFSFileManager.checkModelExists(modelId);
    if (modelExists) {
      console.log(`âœ… [ai-controller] Model ${modelId} found in cache, ready to load`);
      this.modelStatus.state = 4; // ìºì‹œë¨, ë¡œë“œ í•„ìš”
      return true;
    }

    // ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í•„ìš”
    if (modelInfo.requiresToken && !token) {
      console.error(`âŒ [ai-controller] Model ${modelId} requires authentication token`);
      this.modelStatus.error = 'Authentication token required';
      return false;
    }

    // ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    console.log(`ğŸ“¥ [ai-controller] Downloading model ${modelId}...`);
    return this.downloadAndCacheModel(token || '', modelId);
  }

  /**
   * ëª¨ë“  ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ë° ìƒíƒœ ë°˜í™˜
   */
  public async getAllModelsStatus(): Promise<Record<string, { exists: boolean; size?: number }>> {
    const result: Record<string, { exists: boolean; size?: number }> = {};

    for (const modelId of Object.keys(AVAILABLE_MODELS)) {
      try {
        const exists = await OPFSFileManager.checkModelExists(modelId);
        result[modelId] = { exists };

        if (exists) {
          // íŒŒì¼ í¬ê¸° í™•ì¸ì€ OPFSFileManagerì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ìœ„ì„
          // ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
        }
      } catch (error) {
        console.error(`âŒ [ai-controller] Failed to check model ${modelId}:`, error);
        result[modelId] = { exists: false };
      }
    }

    return result;
  }

  /**
   * íŠ¹ì • ëª¨ë¸ì˜ ëŒ€ëµì ì¸ ì»¤ë²„ë¦¬ì§€ í‰ê°€ (ì˜ˆì‹œìš©)
   */
  public getModelCapabilities(modelId: string): {
    accuracy: string;
    speed: string;
    memoryUsage: string;
    authRequired: boolean;
  } | null {
    const modelInfo = AVAILABLE_MODELS[modelId];
    if (!modelInfo) return null;

    // ëª¨ë¸ë³„ ëŒ€ëµì ì¸ íŠ¹ì„± ë§¤í•‘
    const capabilities = {
      'gemma3-4b-it': {
        accuracy: 'ë†’ìŒ (ìµœê³  95.7%)',
        speed: 'ë³´í†µ (347ms)',
        memoryUsage: 'ë†’ìŒ (2.4GB)',
        authRequired: true
      },
      'phi-4-mini': {
        accuracy: 'ë†’ìŒ (ì¶”ì • 90%+)',
        speed: 'ë¹ ë¦„ (ì¶”ì • 280ms)',
        memoryUsage: 'ë³´í†µ (1.8GB)',
        authRequired: false
      }
    };

    return capabilities[modelId as keyof typeof capabilities] || {
      accuracy: 'ì•Œ ìˆ˜ ì—†ìŒ',
      speed: 'ì•Œ ìˆ˜ ì—†ìŒ',
      memoryUsage: modelInfo.size,
      authRequired: modelInfo.requiresToken
    };
  }
}

let aiControllerInstance: AIController | null = null;

/**
 * AI ì»¸íŠ¸ë¡¤ëŸ¬ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›)
 */
export function getAIController(modelId?: string): AIController {
  if (!aiControllerInstance) {
    aiControllerInstance = new AIController({}, modelId);
  } else if (modelId && modelId !== aiControllerInstance.getCurrentModelId()) {
    // ë‹¤ë¥¸ ëª¨ë¸ì´ ìš”ì²­ë˜ë©´ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    console.log(`ğŸ”„ [ai-controller] Creating new instance for model: ${modelId}`);
    aiControllerInstance = new AIController({}, modelId);
  }
  return aiControllerInstance;
}

/**
 * AI ì»¸íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°•ì œ ì¬ì„¤ì • (ë””ë²„ê¹…ìš©)
 */
export function resetAIController(): void {
  if (aiControllerInstance) {
    console.log('ğŸ”„ [ai-controller] Resetting AI controller instance');
    aiControllerInstance = null;
  }
}