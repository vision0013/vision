// AI ì¶”ë¡  ì»¨íŠ¸ë¡¤ëŸ¬ - Gemma 3 4B ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ

import { LlmInference, FilesetResolver } from '@mediapipe/tasks-genai';
import { VoiceIntent, AIAnalysisResult, AIModelConfig, AIModelStatus } from '../types/ai-types';
import { getPromptTemplate, AI_PROMPTS, CURRENT_PROMPT } from '../config/ai-prompts';

// OPFS ì„¤ì • (IndexedDB ì œê±°, OPFSë§Œ ì‚¬ìš©)
const MODEL_KEY = 'gemma3-4b-it-int4';
const MODEL_FILE_NAME = `${MODEL_KEY}.bin`;

export class AIController {
  private llm: LlmInference | null = null;
  private modelStatus: AIModelStatus = {
    state: 1 // 1: ìºì‹œì—†ìŒ/ë¡œë”©ì•ˆë¨
  };

  private readonly fullConfig: AIModelConfig;
  
  // âœ¨ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ - CURRENT_PROMPT ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸°í™”
  private currentPromptName: keyof typeof AI_PROMPTS;


  // AI ì¶”ë¡  ë™ì‹œì„± ì œì–´
  private isAnalyzing = false;
  private analysisQueue: Array<{
    voiceInput: string;
    resolve: (result: AIAnalysisResult) => void;
    reject: (error: Error) => void;
  }> = [];


  constructor(config: AIModelConfig = {}) {
    this.fullConfig = {
      // [ë³€ê²½] Gemma3-4B-IT ëª¨ë¸ ìœ ì§€ (ì‚¬ìš©ì ìš”ì²­)
      // IndexedDB ìŠ¤íŠ¸ë¦¬ë° ì €ì¥ ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°
modelPath: "https://huggingface.co/litert-community/Gemma3-4B-IT/resolve/main/gemma3-4b-it-int4-web.task",
      maxTokens: 2048,
      // âœ¨ [ìˆ˜ì •] ì˜¨ë„ ê°’ì„ ë‚®ì¶° ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ
      temperature: 0.05,
      topK: 40,
      randomSeed: 42,
      ...config
    };

    // âœ¨ CURRENT_PROMPTì—ì„œ í‚¤ ì°¾ê¸°
    const currentKey = Object.keys(AI_PROMPTS).find(
      key => AI_PROMPTS[key as keyof typeof AI_PROMPTS] === CURRENT_PROMPT
    ) as keyof typeof AI_PROMPTS;
    
    this.currentPromptName = currentKey || 'SIMPLE_CLASSIFICATION';
    
    console.log(`ğŸ¤– [ai-controller] Config initialized for API token download.`);
    console.log(`ğŸ¯ [ai-controller] Using prompt: ${this.currentPromptName} (${AI_PROMPTS[this.currentPromptName].name})`);
  }

  /**
   * MediaPipe LLM ì´ˆê¸°í™” (ë¡œì»¬ ìºì‹œ ìš°ì„ )
   */
  async initialize(): Promise<boolean> {
    if (this.modelStatus.state === 3) return true; // ì´ë¯¸ ë¡œë“œë¨
    if (this.modelStatus.state === 2) return false; // ë¡œë”© ì¤‘

    try {
      this.modelStatus.state = 2; // ë¡œë”© ì¤‘
      const startTime = Date.now();
      console.log('ğŸš€ [ai-controller] Initializing AI model from OPFS cache...');

      // OPFSì—ì„œ ëª¨ë¸ íŒŒì¼ í™•ì¸ (ë” ìƒì„¸í•œ ë¡œê¹…)
      console.log('ğŸ” [ai-controller] Checking if model exists in OPFS...');
      const modelExists = await this.checkModelExistsInOPFS();
      console.log(`ğŸ” [ai-controller] Model exists check result: ${modelExists}`);
      
      if (!modelExists) {
        console.error('âŒ [ai-controller] No model file found in OPFS. Please download the model.');
        this.modelStatus = { state: 1, error: 'Model not found in OPFS.' };
        return false;
      }

      try {
        console.log('ğŸ”— [ai-controller] Loading model from OPFS file...');
        
        // OPFSì—ì„œ íŒŒì¼ ë¡œë“œí•˜ì—¬ Object URL ìƒì„±
        const modelFilePath = await this.getModelFileURL();
        console.log(`âœ… [ai-controller] Model file URL: ${modelFilePath}`);
        
        const wasmPath = chrome.runtime.getURL("wasm_files/");
        const genaiFileset = await FilesetResolver.forGenAiTasks(wasmPath);
        
        // modelAssetPath ë°©ì‹ìœ¼ë¡œ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        this.llm = await LlmInference.createFromOptions(genaiFileset, {
          baseOptions: { modelAssetPath: modelFilePath },
            maxTokens: this.fullConfig.maxTokens!,
            temperature: this.fullConfig.temperature!,
            topK: this.fullConfig.topK!,
            randomSeed: this.fullConfig.randomSeed!
          });
          
          console.log('âœ… [ai-controller] Model loaded from file path successfully');
          
      } catch (loadError: any) {
        console.error('âŒ [ai-controller] Failed to load model from OPFS:', loadError);
        this.modelStatus = { state: 1, error: loadError.message };
        throw loadError;
      }

      const loadTime = Date.now() - startTime;
      this.modelStatus = {
        state: 3, // ë¡œë”© ì™„ë£Œ
        loadTime: loadTime
      };

      console.log(`âœ… [ai-controller] SUCCESS: Model loaded from .task bundle in ${loadTime}ms`);
      return true;

    } catch (error: any) {
      this.modelStatus = { state: 1, error: error.message };
      console.error('âŒ [ai-controller] FAILED to initialize from cache:', error);
      return false;
    }
  }

  /**
   * API í† í°ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹±
   */
  /**
   * ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤ì œ ì €ì¥ (modelAssetPath ìŠ¤í‚µí•˜ê³  ë°”ë¡œ ë‹¤ìš´ë¡œë“œ)
   */
  async downloadAndCacheModelAsPath(token: string): Promise<boolean> {
    console.log('ğŸ’¾ [ai-controller] Skipping modelAssetPath, downloading and caching model for persistent storage...');
    
    // modelAssetPathëŠ” ì‹¤ì œ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì‚¬ìš©
    return this.downloadAndCacheModel(token);
  }

  async downloadAndCacheModel(token: string): Promise<boolean> {
    console.log('ğŸ“¥ [ai-controller] Downloading model from Hugging Face with API token...');
    this.modelStatus = { state: 2 }; // ë¡œë”© ì¤‘
    
    // AbortControllerë¡œ ê¸´ ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì œì–´
    const controller = new AbortController();
    const downloadTimeout = setTimeout(() => {
      controller.abort();
      console.error('âŒ [ai-controller] Download timeout after 10 minutes');
    }, 10 * 60 * 1000); // 10ë¶„ íƒ€ì„ì•„ì›ƒ

    try {
      console.log('ğŸ”— [ai-controller] Attempting to fetch:', this.fullConfig.modelPath);
      console.log('ğŸ”‘ [ai-controller] Using token:', token ? `${token.substring(0, 10)}...` : 'No token');
      
      const response = await fetch(this.fullConfig.modelPath!, {
        headers: {
          Authorization: `Bearer ${token}`,
          'User-Agent': 'Chrome Extension Crawler v4.16.0',
        },
        signal: controller.signal // AbortController ì‹ í˜¸ ì¶”ê°€
      });

      // íƒ€ì„ì•„ì›ƒ í•´ì œ
      clearTimeout(downloadTimeout);

      console.log('ğŸ“¡ [ai-controller] Response status:', response.status, response.statusText);
      console.log('ğŸ“¡ [ai-controller] Response headers:', Object.fromEntries(response.headers.entries()));

      if (!response.ok) {
        throw new Error(`Failed to download model: ${response.status} ${response.statusText}`);
      }
      
      console.log('ğŸ“Š [ai-controller] Starting model download to OPFS, this may take several minutes...');
      
      // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ OPFSì— ì§ì ‘ ì €ì¥ (ë©”ëª¨ë¦¬ ì•ˆì „)
      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('Response body is not readable');
      }

      const contentLength = parseInt(response.headers.get('content-length') || '0');
      console.log(`ğŸ“¦ [ai-controller] Expected size: ${(contentLength / 1024 / 1024).toFixed(2)}MB`);

      // OPFS íŒŒì¼ í•¸ë“¤ ìƒì„±
      const { writable } = await this.createOPFSFileWriter();
      let receivedLength = 0;

      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          
          // OPFSì— ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°
          await writable.write(value);
          receivedLength += value.length;
          
          // ì§„í–‰ë¥  ë¡œê·¸ (100MBë§ˆë‹¤)
          if (receivedLength % (100 * 1024 * 1024) < value.length) {
            const progress = contentLength > 0 ? (receivedLength / contentLength * 100).toFixed(1) : 'unknown';
            console.log(`ğŸ“Š [ai-controller] Downloaded to OPFS: ${(receivedLength / 1024 / 1024).toFixed(2)}MB (${progress}%)`);
          }
        }

        // íŒŒì¼ ì“°ê¸° ì™„ë£Œ
        await writable.close();
        console.log(`âœ… [ai-controller] Download complete to OPFS: ${(receivedLength / 1024 / 1024).toFixed(2)}MB`);
        
        // ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ (ë¡œë“œí•˜ì§€ ì•ŠìŒ)
        this.modelStatus = { 
          state: 4 // ìºì‹œë¨, ë¡œë“œ í•„ìš”
        };
        console.log('âœ… [ai-controller] Model status updated: Ready to load');
        
        return true;
        
      } catch (writeError) {
        // ì“°ê¸° ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì •ë¦¬
        await writable.abort();
        throw writeError;
      }
  
    } catch (error: any) {
      clearTimeout(downloadTimeout); // ì—ëŸ¬ ì‹œì—ë„ íƒ€ì„ì•„ì›ƒ í•´ì œ
      
      console.error('âŒ [ai-controller] Full error details:', {
        name: error.name,
        message: error.message,
        stack: error.stack
      });
      
      if (error.name === 'AbortError') {
        console.error('âŒ [ai-controller] Download aborted due to timeout (10 minutes)');
        this.modelStatus = { state: 1, error: 'Download timeout after 10 minutes' };
      } else if (error.message.includes('Failed to fetch')) {
        console.error('âŒ [ai-controller] Network error during download. Possible causes:');
        console.error('  1. Invalid Hugging Face token');
        console.error('  2. Token lacks access to this model');
        console.error('  3. Network/firewall blocking the request');
        console.error('  4. CORS policy restriction');
        this.modelStatus = { state: 1, error: 'Network connection failed. Check your internet connection or Hugging Face token.' };
      } else {
        console.error('âŒ [ai-controller] Failed to download or cache model:', error);
        this.modelStatus = { state: 1, error: error.message };
      }
      return false;
    }
  }
  
  /**
   * OPFS ìºì‹œëœ ëª¨ë¸ ì‚­ì œ
   */
  async deleteCachedModel(): Promise<void> {
    try {
      const opfsRoot = await navigator.storage.getDirectory();
      const modelsDir = await opfsRoot.getDirectoryHandle('models', { create: false });
      await modelsDir.removeEntry(MODEL_FILE_NAME);
      console.log('âœ… [ai-controller] OPFS model file deleted.');
    } catch (error: any) {
      if (error.name === 'NotFoundError') {
        console.log('â„¹ï¸ [ai-controller] Model file not found in OPFS (already deleted).');
      } else {
        console.error('âŒ [ai-controller] Failed to delete OPFS model:', error);
        throw error;
      }
    }
    
    if (this.llm) this.llm = null;
    this.modelStatus = { state: 1 }; // ìºì‹œ ì—†ìŒ
  }

  /**
   * OPFSì—ì„œ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ë§Œ ì²´í¬ (ë¡œë”©í•˜ì§€ ì•ŠìŒ)
   */
  async checkModelExistsInOPFS(): Promise<boolean> {
    // ë‹¤ìš´ë¡œë“œ ì¤‘ì´ì–´ë„ ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ëŠ” ì²´í¬ (ë” ì•ˆì „í•¨)
    console.log(`ğŸ” [ai-controller] Checking OPFS file existence (current state: ${this.modelStatus.state})`);
    if (this.modelStatus.state === 2) {
      console.log('âš ï¸ [ai-controller] Download in progress, but checking file anyway...');
    }

    try {
      const opfsRoot = await navigator.storage.getDirectory();
      const modelsDir = await opfsRoot.getDirectoryHandle('models', { create: false });
      const fileHandle = await modelsDir.getFileHandle(MODEL_FILE_NAME, { create: false });
      const file = await fileHandle.getFile();
      
      const exists = file.size > 0;
      console.log(`ğŸ” [ai-controller] Found model in OPFS: ${(file.size / 1024 / 1024).toFixed(2)}MB`);
      return exists;
    } catch (error: any) {
      if (error.name === 'NotFoundError') {
        console.log('ğŸ” [ai-controller] No model file found in OPFS');
        return false;
      } else {
        console.error('âŒ [ai-controller] Failed to check OPFS model existence:', error);
        return false;
      }
    }
  }

  /**
   * ëª¨ë¸ ìƒíƒœ í™•ì¸ (OPFS ì¡´ì¬ ì—¬ë¶€ ë°˜ì˜)
   */
  async getModelStatus(): Promise<AIModelStatus> {
    if (this.modelStatus.state === 1) {
      const modelExists = await this.checkModelExistsInOPFS();
      if (modelExists) {
        this.modelStatus = {
          ...this.modelStatus,
          state: 4 
        };
        console.log('ğŸ“¦ [ai-controller] Model found in cache but not loaded in memory');
      }
    }
    return { ...this.modelStatus };
  }

  /**
   * ìŒì„± ëª…ë ¹ ì˜ë„ ë¶„ì„ (ë™ì‹œì„± ì œì–´ í¬í•¨)
   */
  async analyzeIntent(voiceInput: string): Promise<AIAnalysisResult> {
    console.log('ğŸ¯ [ai-controller] Analyzing voice intent with Gemma 3 4B:', voiceInput);

    if (this.modelStatus.state !== 3 || !this.llm) {
      console.log('âš ï¸ [ai-controller] Model not loaded, using fallback analysis');
      throw new Error('AI model is not initialized.');
    }

    // ë™ì‹œì„± ì œì–´: íì— ì¶”ê°€í•˜ê³  ìˆœì°¨ ì²˜ë¦¬
    return new Promise((resolve, reject) => {
      this.analysisQueue.push({ voiceInput, resolve, reject });
      this.processAnalysisQueue();
    });
  }

  private async processAnalysisQueue(): Promise<void> {
    if (this.isAnalyzing || this.analysisQueue.length === 0) {
      return;
    }

    this.isAnalyzing = true;

    while (this.analysisQueue.length > 0) {
      const { voiceInput, resolve, reject } = this.analysisQueue.shift()!;

      try {
        console.log(`ğŸ”„ [ai-controller] Processing analysis (${this.analysisQueue.length} remaining)`);
        const prompt = this.buildAnalysisPrompt(voiceInput);
        const response = await this.llm!.generateResponse(prompt);
        const result = this.parseAIResponse(response, voiceInput);
        resolve(result);
      } catch (error: any) {
        console.error('âŒ [ai-controller] AI analysis failed:', error);
        reject(error);
      }
    }

    this.isAnalyzing = false;
  }

  /**
   * âœ¨ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë©”ì„œë“œë“¤
   */
  public setPromptTemplate(promptName: keyof typeof AI_PROMPTS): void {
    this.currentPromptName = promptName;
    console.log(`ğŸ”„ [ai-controller] Switched to prompt: ${AI_PROMPTS[promptName].name}`);
  }

  public getCurrentPrompt(): string {
    return AI_PROMPTS[this.currentPromptName].name;
  }

  public getAvailablePrompts(): Array<{name: keyof typeof AI_PROMPTS, description: string}> {
    return Object.entries(AI_PROMPTS).map(([key, value]) => ({
      name: key as keyof typeof AI_PROMPTS,
      description: value.description
    }));
  }

  /**
   * âœ¨ ì„¤ì • íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (CURRENT_PROMPT ê¸°ë°˜)
   */
  private buildAnalysisPrompt(voiceInput: string): string {
    const promptTemplate = getPromptTemplate(this.currentPromptName);
    console.log(`ğŸ¯ [ai-controller] Using prompt template: ${promptTemplate.name}`);
    return promptTemplate.template(voiceInput);
  }

  /**
   * âœ¨ ì•ˆì •ì ì¸ íŒŒì‹± ë¡œì§
   */
  private parseAIResponse(response: string, originalCommand: string): AIAnalysisResult {
    try {
      console.log('ğŸ” [ai-controller] Raw AI response:', response);
      
      const firstBrace = response.indexOf('{');
      const lastBrace = response.lastIndexOf('}');
      
      if (firstBrace === -1 || lastBrace === -1 || lastBrace < firstBrace) {
        console.warn('âš ï¸ [ai-controller] No valid JSON object found in response, using fallback.');
        const fallbackAction = this.guessActionFromText(originalCommand);
        const intent: VoiceIntent = {
          action: fallbackAction,
          confidence: 0.8,
          reasoning: 'Fallback analysis (No JSON found)'
        };
        return { intent, reasoning: 'Fallback analysis (No JSON found)' };
      }
      
      let jsonString = response.substring(firstBrace, lastBrace + 1);
      
      // âœ¨ JSON ì •ë¦¬ ë¡œì§
      jsonString = this.sanitizeJsonString(jsonString);
      
      const parsedResponse = JSON.parse(jsonString);
      
      const intent: VoiceIntent = {
        action: parsedResponse.action || 'unknown',
        product: parsedResponse.product,
        target: parsedResponse.target,
        detail: parsedResponse.detail,
        confidence: parsedResponse.confidence || 0.8,
        reasoning: parsedResponse.reasoning || 'AI analysis complete'
      };
      
      return {
        intent,
        reasoning: parsedResponse.reasoning || 'AI analysis complete',
      };
    } catch (error: any) {
      console.error('âŒ [ai-controller] Failed to parse AI response:', error);
      console.error('âŒ [ai-controller] Response was:', response);
      
      // âœ¨ fallback ì²˜ë¦¬
      const fallbackAction = this.guessActionFromText(originalCommand);
      const intent: VoiceIntent = {
        action: fallbackAction,
        confidence: 0.7,
        reasoning: 'Fallback analysis (JSON parsing failed)'
      };
      return { intent, reasoning: 'Fallback analysis (JSON parsing failed)' };
    }
  }

  /**
   * JSON ë¬¸ìì—´ ì •ë¦¬
   */
  private sanitizeJsonString(jsonString: string): string {
    try {
      // reasoning ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œ ë¬¸ì œ í•´ê²°
      const reasoningMatch = jsonString.match(/"reasoning":\s*"([^"]*(?:"[^"]*"[^"]*)*[^"]*)"/);
      if (reasoningMatch) {
        const originalReasoning = reasoningMatch[1];
        // ë‚´ë¶€ ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½
        const cleanReasoning = originalReasoning.replace(/"/g, "'");
        jsonString = jsonString.replace(reasoningMatch[0], `"reasoning": "${cleanReasoning}"`);
      }
      
      // ê¸°íƒ€ ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì •
      jsonString = jsonString.replace(/[\r\n\t]/g, ' '); // ê°œí–‰ë¬¸ì ì œê±°
      jsonString = jsonString.replace(/,\s*}/g, '}');    // ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±°
      
      console.log('ğŸ”§ [ai-controller] Sanitized JSON:', jsonString);
      return jsonString;
    } catch (error) {
      console.warn('âš ï¸ [ai-controller] JSON sanitization failed:', error);
      return jsonString; // ì›ë³¸ ë°˜í™˜
    }
  }

  /**
   * í´ë°± ë¶„ì„
   */
  private guessActionFromText(text: string): VoiceIntent['action'] {
    const lower = text.toLowerCase();
    console.log('ğŸ” [ai-controller] Fallback analysis for:', lower);
    
    if ((lower.includes('ì•„ì´í°') || lower.includes('ê°¤ëŸ­ì‹œ') || lower.includes('ë…¸íŠ¸ë¶')) && 
        (lower.includes('ì°¾ì•„') || lower.includes('ê²€ìƒ‰'))) {
      return 'product_search';
    }
    if (lower.includes('ìµœì €ê°€') || lower.includes('ê°€ê²©') || lower.includes('ë¹„êµ')) return 'price_comparison';
    if (lower.includes('ë²„íŠ¼') || lower.includes('í´ë¦­') || lower.includes('ëˆŒëŸ¬')) return 'simple_find';
    if (lower.includes('ì¥ë°”êµ¬ë‹ˆ') || lower.includes('êµ¬ë§¤') || lower.includes('ê²°ì œ')) return 'purchase_flow';
    if (lower.includes('ì´ì „') || lower.includes('ë’¤ë¡œ') || lower.includes('ì´ë™')) return 'navigation';
    if (lower.includes('ì°¾ì•„') || lower.includes('ê²€ìƒ‰')) return 'product_search';
    
    return 'unknown';
  }

  /**
   * OPFS íŒŒì¼ ì“°ê¸°ìš© WritableStream ìƒì„±
   */
  private async createOPFSFileWriter(): Promise<{ writable: FileSystemWritableFileStream, fileHandle: FileSystemFileHandle }> {
    try {
      console.log('ğŸ“ [ai-controller] Creating OPFS file writer...');
      
      const opfsRoot = await navigator.storage.getDirectory();
      const modelsDir = await opfsRoot.getDirectoryHandle('models', { create: true });
      const fileHandle = await modelsDir.getFileHandle(MODEL_FILE_NAME, { create: true });
      const writable = await fileHandle.createWritable();
      
      console.log(`ğŸ“„ [ai-controller] Created OPFS writable stream: ${MODEL_FILE_NAME}`);
      return { writable, fileHandle };
      
    } catch (error: any) {
      console.error('âŒ [ai-controller] Failed to create OPFS writer:', error);
      throw new Error(`OPFS writer creation failed: ${error.message}`);
    }
  }

  /**
   * OPFSì—ì„œ ëª¨ë¸ íŒŒì¼ì˜ Object URL ë°˜í™˜
   */
  private async getModelFileURL(): Promise<string> {
    try {
      console.log('ğŸ”— [ai-controller] Getting OPFS root directory...');
      const opfsRoot = await navigator.storage.getDirectory();
      
      console.log('ğŸ”— [ai-controller] Getting models directory...');
      const modelsDir = await opfsRoot.getDirectoryHandle('models', { create: false });
      
      console.log(`ğŸ”— [ai-controller] Getting file handle for: ${MODEL_FILE_NAME}`);
      const fileHandle = await modelsDir.getFileHandle(MODEL_FILE_NAME, { create: false });
      
      console.log('ğŸ”— [ai-controller] Getting file object...');
      const file = await fileHandle.getFile();
      console.log(`ğŸ”— [ai-controller] File size: ${(file.size / 1024 / 1024).toFixed(2)}MB`);
      
      const fileUrl = URL.createObjectURL(file);
      console.log(`ğŸ”— [ai-controller] Created file URL from OPFS: ${fileUrl}`);
      
      return fileUrl;
      
    } catch (error: any) {
      console.error('âŒ [ai-controller] Failed to get OPFS file URL:', error);
      console.error('âŒ [ai-controller] Error details:', {
        name: error.name,
        message: error.message,
        stack: error.stack
      });
      throw new Error(`OPFS file URL creation failed: ${error.message}`);
    }
  }
}

let aiControllerInstance: AIController | null = null;

export function getAIController(): AIController {
  if (!aiControllerInstance) {
    aiControllerInstance = new AIController();
  }
  return aiControllerInstance;
}