// src/features/ai-inference/controllers/ai-controller.ts

import { AIAnalysisResult, AIModelConfig, AIModelStatus, ModelDownloadProgress } from '../types/ai-types';
import { CrawledItem, Mode } from '../../../types';
import { AVAILABLE_MODELS, DEFAULT_MODEL_ID } from '../config/model-registry';
import { ModelManager } from './model-manager';
import { InferenceEngine } from './inference-engine';
import { AI_PROMPTS } from '../config/ai-prompts';

export class AIController {
  private modelManager: ModelManager;
  private inferenceEngine: InferenceEngine;
  private isInitialized: boolean = false; // âœ¨ [ì‹ ê·œ] ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸

  constructor(config: AIModelConfig = {}, modelId?: string) {
    const targetModelId = modelId || DEFAULT_MODEL_ID;
    const modelInfo = AVAILABLE_MODELS[targetModelId];
    const fullConfig = { ...modelInfo.defaultConfig, ...config };

    this.modelManager = new ModelManager(fullConfig, targetModelId);
    // âœ¨ [ìˆ˜ì •] InferenceEngineì— ì»¨íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬
    this.inferenceEngine = new InferenceEngine(null, this);
  }

  async initialize(): Promise<boolean> {
    this.isInitialized = false; // ì´ˆê¸°í™” ì‹œì‘ ì‹œ í”Œë˜ê·¸ ë¦¬ì…‹
    const success = await this.modelManager.initialize();
    if (success) {
      this.inferenceEngine.setLlm(this.modelManager.getLlm());
      this.isInitialized = true; // âœ¨ [ì‹ ê·œ] ì„±ê³µ ì‹œ í”Œë˜ê·¸ ì„¤ì •
    }
    return success;
  }

  // âœ¨ [ì‹ ê·œ] ì¶”ë¡  ê°€ëŠ¥ ìƒíƒœ í™•ì¸ ë©”ì†Œë“œ
  public isReadyForInference(): boolean {
    return this.isInitialized;
  }

  async downloadAndCacheModel(token: string, modelId?: string): Promise<boolean> {
    this.isInitialized = false; // ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ìƒíƒœ í•´ì œ
    return this.modelManager.downloadAndCacheModel(token, modelId);
  }

  async downloadAndCacheModelAsPath(token: string, modelId?: string): Promise<boolean> {
    return this.downloadAndCacheModel(token, modelId);
  }

  cancelDownload(): void {
    this.modelManager.cancelDownload();
  }

  async deleteCachedModel(modelId?: string): Promise<void> {
    await this.modelManager.deleteCachedModel(modelId);
  }

  async getModelStatus(modelId?: string): Promise<AIModelStatus> {
    return this.modelManager.getModelStatus(modelId);
  }

  getDownloadProgress(): ModelDownloadProgress | null {
    return this.modelManager.getDownloadProgress();
  }

  async analyzeIntent(voiceInput: string, crawledItems: CrawledItem[], mode: Mode): Promise<AIAnalysisResult> {
    return this.inferenceEngine.analyzeIntent(voiceInput, crawledItems, mode);
  }

  async analyzeChat(userInput: string): Promise<string> {
    return this.inferenceEngine.analyzeChat(userInput);
  }

  setPromptTemplate(promptName: keyof typeof AI_PROMPTS): void {
    this.inferenceEngine.setPromptTemplate(promptName);
  }

  getCurrentPrompt(): string {
    return this.inferenceEngine.getCurrentPrompt();
  }
  
  getAvailablePrompts() {
    return this.inferenceEngine.getAvailablePrompts();
  }


  getAvailableModels() {
    return AVAILABLE_MODELS;
  }

  getCurrentModelId(): string {
    return this.modelManager.getCurrentModelId();
  }

  isModelLoaded(): boolean {
    return this.modelManager.isModelLoaded();
  }

  async switchModel(modelId: string, token?: string, autoLoad: boolean = false): Promise<boolean> {
    this.isInitialized = false; // ëª¨ë¸ ì „í™˜ ì‹œ ì´ˆê¸°í™” ìƒíƒœ í•´ì œ
    const modelInfo = AVAILABLE_MODELS[modelId];
    if (!modelInfo) return false;

    console.log(`ğŸ”„ [ai-controller] Switching from ${this.getCurrentModelId()} to ${modelId}`);

    await this.modelManager.switchModel(modelId);
    this.inferenceEngine.setLlm(null);

    // ğŸ¯ [ìˆ˜ì •] í™œì„± ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸
    setCurrentActiveModel(modelId);


    const modelExists = await this.modelManager.getModelStatus(modelId).then(s => s.state === 4);
    if (modelExists) {
      if (autoLoad) return this.initialize();
      return true;
    }

    if (modelInfo.requiresToken && !token) return false;

    const downloadSuccess = await this.downloadAndCacheModel(token || '', modelId);
    if (downloadSuccess && autoLoad) {
      return this.initialize();
    }
    return downloadSuccess;
  }
  
  async getAllModelsStatus(): Promise<Record<string, { exists: boolean; size?: number }>> {
    return this.modelManager.getAllModelsStatus();
  }
}

let aiControllerInstance: AIController | null = null;
let currentActiveModelId: string = DEFAULT_MODEL_ID; // í˜„ì¬ í™œì„± ëª¨ë¸ ì¶”ì 

export function getAIController(modelId?: string): AIController {
  const targetModelId = modelId || currentActiveModelId;

  console.log(`ğŸ” [getAIController] Called with modelId: ${modelId}, currentActiveModelId: ${currentActiveModelId}, targetModelId: ${targetModelId}`);
  console.log(`ğŸ” [getAIController] Existing instance model: ${aiControllerInstance?.getCurrentModelId() || 'none'}`);

  // ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ê±°ë‚˜, ë‹¤ë¥¸ ëª¨ë¸ IDë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš° ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  if (!aiControllerInstance || aiControllerInstance.getCurrentModelId() !== targetModelId) {
    console.log(`ğŸ”„ [getAIController] Creating new controller for model: ${targetModelId}`);
    console.log(`ğŸ“Š [getAIController] Previous: ${aiControllerInstance?.getCurrentModelId() || 'none'} â†’ New: ${targetModelId}`);
    aiControllerInstance = new AIController({}, targetModelId);
    currentActiveModelId = targetModelId;
  } else {
    console.log(`âœ… [getAIController] Using existing controller for model: ${targetModelId}`);
  }

  return aiControllerInstance;
}

export function setCurrentActiveModel(modelId: string): void {
  console.log(`ğŸ¯ [setCurrentActiveModel] Setting active model to: ${modelId}`);
  currentActiveModelId = modelId;
  // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ë„ ë¦¬ì…‹í•´ì„œ ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ìƒì„±ë˜ë„ë¡ í•¨
  aiControllerInstance = null;
}

export function resetAIController(): void {
  aiControllerInstance = null;
}