// Content Scriptìš© AI Controller - DOM í™˜ê²½ì—ì„œ MediaPipe ì‹¤í–‰

import { VoiceIntent, AIAnalysisResult, AIModelConfig, AIModelStatus } from '../types/ai-types';

export class ContentAIController {
  private llm: any | null = null; // LlmInference íƒ€ì… - ë™ì  importë¥¼ ìœ„í•´ anyë¡œ ë³€ê²½
  private modelStatus: AIModelStatus = {
    state: 1 // ìºì‹œì—†ìŒ/ë¡œë”©ì•ˆë¨
  };

  private readonly config: AIModelConfig = {
    modelPath: "https://huggingface.co/litert-community/Gemma3-1B-IT/resolve/main/gemma3-1b-it-int4.task?download=true",
    maxTokens: 256,
    temperature: 0.7,
    topK: 40,
    randomSeed: 42
  };

  /**
   * Content Scriptì—ì„œ MediaPipe LLM ì´ˆê¸°í™” (ë™ì  import ì‚¬ìš©)
   */
  async initialize(): Promise<boolean> {
    if (this.modelStatus.state === 3) {
      console.log('ğŸ¤– [content-ai] Model already loaded');
      return true;
    }

    if (this.modelStatus.state === 2) {
      console.log('ğŸ”„ [content-ai] Model is already loading...');
      return false;
    }

    try {
      this.modelStatus.state = 2; // ë¡œë”© ì¤‘
      const startTime = Date.now();
      console.log('ğŸš€ [content-ai] Starting AI model initialization in Content Script...');
      console.log('ğŸ“¦ [content-ai] Model path:', this.config.modelPath);

      // 1. MediaPipe ë™ì  import
      console.log('ğŸ”„ [content-ai] Loading MediaPipe library...');
      const { LlmInference, FilesetResolver } = await import('@mediapipe/tasks-genai');
      console.log('âœ… [content-ai] MediaPipe library loaded');

      // 2. MediaPipe WASM íŒŒì¼ì…‹ ë¡œë“œ
      console.log('ğŸ”„ [content-ai] Loading MediaPipe WASM files...');
      const genaiFileset = await FilesetResolver.forGenAiTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@latest/wasm"
      );
      console.log('âœ… [content-ai] WASM files loaded');
      
      // 3. Gemma-3 1B ëª¨ë¸ ë¡œë“œ (ìë™ ë‹¤ìš´ë¡œë“œ)
      console.log('ğŸ”„ [content-ai] Loading Gemma-3 1B model (529MB)...');
      console.log('ğŸ’¡ [content-ai] This may take 30-60 seconds on first load');
      
      this.llm = await LlmInference.createFromOptions(genaiFileset, {
        baseOptions: { 
          modelAssetPath: this.config.modelPath
        },
        maxTokens: this.config.maxTokens,
        temperature: this.config.temperature,
        topK: this.config.topK,
        randomSeed: this.config.randomSeed
      });

      const loadTime = Date.now() - startTime;
      this.modelStatus = {
        state: 3, // ë¡œë”© ì™„ë£Œ
        modelSize: 529 * 1024 * 1024, // 529MB
        loadTime: loadTime
      };

      console.log(`âœ… [content-ai] Gemma-3 1B model loaded successfully in ${loadTime}ms`);
      console.log('ğŸ¯ [content-ai] Ready for AI-powered voice command analysis');
      return true;

    } catch (error: any) {
      const loadTime = Date.now();
      this.modelStatus = {
        state: 1, // ìºì‹œì—†ìŒ/ì‹¤íŒ¨
        error: error.message,
        loadTime: loadTime
      };
      
      console.error('âŒ [content-ai] Failed to initialize AI model:', error);
      console.log('ğŸ”„ [content-ai] Will fallback to oktjs analysis');
      return false;
    }
  }

  /**
   * ìŒì„± ëª…ë ¹ ì˜ë„ ë¶„ì„
   */
  async analyzeIntent(voiceInput: string): Promise<AIAnalysisResult> {
    console.log('ğŸ¯ [content-ai] Analyzing voice intent with Gemma-3 1B:', voiceInput);

    if (this.modelStatus.state !== 3 || !this.llm) {
      console.log('âš ï¸ [content-ai] Model not loaded');
      throw new Error('AI model not loaded');
    }

    try {
      const prompt = this.buildAnalysisPrompt(voiceInput);
      console.log('ğŸ“ [content-ai] Sending prompt to Gemma-3 1B...');
      
      const response = await this.llm.generateResponse(prompt);
      console.log('ğŸ¤– [content-ai] AI response received:', response);
      
      return this.parseAIResponse(response, voiceInput);

    } catch (error: any) {
      console.error('âŒ [content-ai] AI analysis failed:', error);
      throw error;
    }
  }

  private buildAnalysisPrompt(voiceInput: string): string {
    return `ë‹¹ì‹ ì€ ì›¹ ë¸Œë¼ìš°ì € ìŒì„± ëª…ë ¹ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìŒì„± ëª…ë ¹ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ëª…ë ¹: "${voiceInput}"

ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
1. price_comparison: ê°€ê²© ë¹„êµ (ì˜ˆ: "ìµœì €ê°€", "ê°€ê²© ë¹„êµ", "ë” ì‹¼ ê±°", "í• ì¸")
2. product_search: ìƒí’ˆ ê²€ìƒ‰ (ì˜ˆ: "ì°¾ì•„ì¤˜", "ê²€ìƒ‰í•´ì¤˜", "ë³´ì—¬ì¤˜")
3. simple_find: í˜ì´ì§€ ë‚´ ìš”ì†Œ ì°¾ê¸° (ì˜ˆ: "ë²„íŠ¼", "ë§í¬", "ë©”ë‰´", "í´ë¦­")
4. purchase_flow: êµ¬ë§¤ ê´€ë ¨ (ì˜ˆ: "êµ¬ë§¤", "ê²°ì œ", "ì¥ë°”êµ¬ë‹ˆ", "ì£¼ë¬¸")
5. navigation: í˜ì´ì§€ ì´ë™ (ì˜ˆ: "ì´ì „", "ë‹¤ìŒ", "í™ˆ", "ë’¤ë¡œ")

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
  "action": "ì¹´í…Œê³ ë¦¬",
  "product": "ìƒí’ˆëª… (ìˆë‹¤ë©´)",
  "target": "ëŒ€ìƒ ìš”ì†Œ (ìˆë‹¤ë©´)",
  "detail": "êµ¬ì²´ì  ìš”ì²­ì‚¬í•­",
  "confidence": 0.9,
  "reasoning": "íŒë‹¨ ê·¼ê±°"
}`;
  }

  private parseAIResponse(response: string, _originalInput: string): AIAnalysisResult {
    try {
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error('No JSON found in AI response');
      }
      
      const parsedResponse = JSON.parse(jsonMatch[0]);
      
      const intent: VoiceIntent = {
        action: this.normalizeAction(parsedResponse.action),
        product: parsedResponse.product,
        target: parsedResponse.target,
        detail: parsedResponse.detail,
        confidence: parsedResponse.confidence || 0.8
      };
      
      return {
        intent,
        reasoning: parsedResponse.reasoning || 'AI ë¶„ì„ ì™„ë£Œ',
        suggestions: []
      };
      
    } catch (error: any) {
      console.error('âŒ [content-ai] Failed to parse AI response:', error);
      throw error;
    }
  }

  private normalizeAction(aiAction: string): VoiceIntent['action'] {
    const action = aiAction?.toLowerCase() || '';
    
    if (['price_comparison', 'price_compare'].includes(action)) return 'price_comparison';
    if (['product_search', 'search'].includes(action)) return 'product_search';
    if (['simple_find', 'find', 'click'].includes(action)) return 'simple_find';
    if (['purchase_flow', 'purchase', 'buy'].includes(action)) return 'purchase_flow';
    if (['navigation', 'navigate', 'move'].includes(action)) return 'navigation';
    
    return 'unknown';
  }

  getModelStatus(): AIModelStatus {
    return { ...this.modelStatus };
  }

  async dispose(): Promise<void> {
    if (this.llm) {
      this.llm = null;
    }
    
    this.modelStatus = {
      state: 1 // ìºì‹œì—†ìŒ
    };
    
    console.log('ğŸ—‘ï¸ [content-ai] AI model disposed');
  }
}

// Content Scriptìš© ì‹±ê¸€í†¤
let contentAIController: ContentAIController | null = null;

export function getContentAIController(): ContentAIController {
  if (!contentAIController) {
    contentAIController = new ContentAIController();
  }
  return contentAIController;
}