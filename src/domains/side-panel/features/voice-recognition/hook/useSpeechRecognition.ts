import { useState, useEffect, useRef, useCallback } from 'react';

// --- íƒ€ì… ì •ì˜ ---
interface SpeechRecognitionEvent extends Event {
  readonly resultIndex: number;
  readonly results: SpeechRecognitionResultList;
  readonly error?: string;
}
interface CustomSpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  onend: (() => void) | null;
  onresult: ((event: SpeechRecognitionEvent) => void) | null;
  onerror: ((event: SpeechRecognitionEvent) => void) | null;
  start: () => void;
  stop: () => void;
}
interface SpeechRecognitionStatic {
  new(): CustomSpeechRecognition;
}
declare global {
  interface Window {
    SpeechRecognition: SpeechRecognitionStatic;
    webkitSpeechRecognition: SpeechRecognitionStatic;
  }
}

const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

export const useSpeechRecognition = (onCommand: (command: string) => void) => {
  const [isListening, setIsListening] = useState(false);
  const [transcribedText, setTranscribedText] = useState('');
  const [error, setError] = useState<string | null>(null); // ì—ëŸ¬ ìƒíƒœ
  const recognitionRef = useRef<CustomSpeechRecognition | null>(null);
  const isListeningRef = useRef(isListening);

  useEffect(() => {
    isListeningRef.current = isListening;
  }, [isListening]);

  const onCommandRef = useRef(onCommand);
  
  // onCommandê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    onCommandRef.current = onCommand;
  }, [onCommand]);

  useEffect(() => {
    if (!SpeechRecognition) {
      console.error("âŒ SpeechRecognition APIë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
      setError('not-supported');
      return;
    }

    console.log("ğŸ¤ Creating new SpeechRecognition instance");
    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let finalTranscript = '';
      let interimTranscript = '';
      for (let i = event.resultIndex; i < event.results.length; i++) {
        if (event.results[i].isFinal) {
          finalTranscript += event.results[i][0].transcript;
        } else {
          interimTranscript += event.results[i][0].transcript;
        }
      }
      setTranscribedText(interimTranscript);
      if (finalTranscript) {
        console.log("ğŸ¤ Final transcript:", finalTranscript.trim());
        onCommandRef.current(finalTranscript.trim()); // refë¥¼ í†µí•´ í˜¸ì¶œ
      }
    };
    
    recognition.onerror = (event: SpeechRecognitionEvent) => {
      console.error("ğŸš¨ 'onerror' ì´ë²¤íŠ¸ ë°œìƒ: ìŒì„± ì¸ì‹ ì˜¤ë¥˜", event.error);
      setError(event.error || 'unknown-error');
      if (event.error === 'not-allowed') {
        setIsListening(false);
      }
    };

    recognition.onend = () => {
      console.log("ğŸ¤ Recognition ended, restarting if still listening:", isListeningRef.current);
      if (isListeningRef.current) {
        try {
          recognition.start();
        } catch (e) {
          console.error("ğŸš¨ Failed to restart recognition:", e);
        }
      }
    };

    recognitionRef.current = recognition;

    return () => {
      console.log("ğŸ¤ Cleaning up SpeechRecognition");
      if (recognitionRef.current) {
        recognitionRef.current.onend = null;
        recognitionRef.current.stop();
      }
    };
  }, []); // ì˜ì¡´ì„± ì œê±°ë¡œ í•œ ë²ˆë§Œ ìƒì„±

  const toggleListening = useCallback(() => {
    if (!recognitionRef.current) {
        return;
    }

    const newIsListening = !isListening;
    setIsListening(newIsListening);

    if (newIsListening) {
      setError(null); // ì¸ì‹ ì‹œì‘ ì‹œ ì—ëŸ¬ ì´ˆê¸°í™”
      setTranscribedText('');
      recognitionRef.current.start();
    } else {
      recognitionRef.current.stop();
    }
  }, [isListening]);

  // ë°˜í™˜ ê°’ì— 'error'ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
  return { transcribedText, isListening, toggleListening, error };
};
